{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "import pandas as pd\n",
    "import glob, os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "from matplotlib.dates import DateFormatter\n",
    "%matplotlib inline\n",
    "# for applying kfold validation set\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "# for logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# convert date into float\n",
    "import datetime as dt \n",
    "# converting target to labels\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print the channel list\n",
    "#channel_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for the purpose of testing\n",
    "# setting the channel list \n",
    "#channel_list = [103222, 9000, 55227,54454, 51181]\n",
    "channel_list = [56269, 9000, 50959, 55227, 54454, 51181, 103222]\n",
    "#channel_list = [51181, 54454]\n",
    "#channel_list = [9000]\n",
    "# good results with 10 fold: 55227, 9000, 54454, 51181\n",
    "\n",
    "# error with 56269, 50959: RESOLVED\n",
    "# This solver needs samples of at least 2 classes in the data, \n",
    "# but the data contains only one class: 0. \n",
    "\n",
    "# error with 103222: RESOLVED\n",
    "# Cannot have number of splits n_splits=10 greater than the number of samples: 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read layer3-60 data from all the csv files \n",
    "path = r'C:/Users/jainsac/Downloads/MRP/Assignment 2/Data'\n",
    "data = pd.concat(map(lambda file: pd.read_csv(file, header = None), glob.glob(os.path.join('', path+'/*.csv'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StartTime</th>\n",
       "      <th>EndTime</th>\n",
       "      <th>Channel</th>\n",
       "      <th>Power_dbm</th>\n",
       "      <th>SNR</th>\n",
       "      <th>Occupancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-10-06 00:00:00.1640000</td>\n",
       "      <td>2016-10-06 01:00:00.0000000</td>\n",
       "      <td>56302</td>\n",
       "      <td>-77.940605</td>\n",
       "      <td>26.758839</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-10-06 00:00:00.1640000</td>\n",
       "      <td>2016-10-06 01:00:00.0000000</td>\n",
       "      <td>56303</td>\n",
       "      <td>-83.293709</td>\n",
       "      <td>21.397917</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-10-06 00:00:00.1640000</td>\n",
       "      <td>2016-10-06 01:00:00.0000000</td>\n",
       "      <td>56304</td>\n",
       "      <td>-63.193748</td>\n",
       "      <td>41.497952</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     StartTime                      EndTime  Channel  \\\n",
       "0  2016-10-06 00:00:00.1640000  2016-10-06 01:00:00.0000000    56302   \n",
       "1  2016-10-06 00:00:00.1640000  2016-10-06 01:00:00.0000000    56303   \n",
       "2  2016-10-06 00:00:00.1640000  2016-10-06 01:00:00.0000000    56304   \n",
       "\n",
       "   Power_dbm        SNR  Occupancy  \n",
       "0 -77.940605  26.758839      100.0  \n",
       "1 -83.293709  21.397917      100.0  \n",
       "2 -63.193748  41.497952      100.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.columns = ['StartTime', 'EndTime', 'Channel', 'Power_dbm','SNR', 'Occupancy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plt.hist(data['Occupancy'])\n",
    "# plt.xlabel(\"Occupancy\")\n",
    "# plt.ylabel(\"Frequency\")\n",
    "# plt.title(\"Histogram for Occupancy attribute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plt.hist(data['SNR'])\n",
    "# plt.xlabel(\"SNR\")\n",
    "# plt.ylabel(\"Frequency\")\n",
    "# plt.title(\"Histogram for SNR attribute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plt.hist(data['Power_dbm'])\n",
    "# plt.xlabel(\"Power_dbm\")\n",
    "# plt.ylabel(\"Frequency\")\n",
    "# plt.title(\"Histogram for Power_dbm attribute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# corr = data.corr()\n",
    "# sns.heatmap(corr, \n",
    "#             xticklabels=corr.columns.values,\n",
    "#             yticklabels=corr.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data['Power_dbm'].corr(data['SNR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copying the df to a new dataframe so that \n",
    "# future processing could be done easily\n",
    "Temporaldf = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "channel_list = data.Channel.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#channel_list = [54048, 56302, 9000, 50959, 55227, 54454, 51181]\n",
    "#channel_list = [9000]\n",
    "# for the Temporaldf take start time and sort it and \n",
    "\n",
    "uniqueST = Temporaldf.StartTime.unique()\n",
    "ST_df=pd.DataFrame(uniqueST, columns=['StartT']) \n",
    "TemporalDFInd = ST_df.set_index('StartT')\n",
    "TempReindex = TemporalDFInd.reindex(pd.date_range(start=TemporalDFInd.index[0], end=TemporalDFInd.index[-1], freq='1h'))\n",
    "reindexed = TempReindex.shape[0]\n",
    "\n",
    "channelO = []\n",
    "for ch in channel_list:\n",
    "    TemporalDF = Temporaldf[(Temporaldf.Channel == ch)][['StartTime']]\n",
    "    #print(TemporalDF)\n",
    "    #TemporalDFInd = TemporalDF.set_index('StartTime')\n",
    "    original = TemporalDF.shape[0]\n",
    "    #print(original)\n",
    "    #TempReindex = TemporalDFInd.reindex(pd.date_range(start=TemporalDFInd.index[0], end=TemporalDFInd.index[-1], freq='1h'))\n",
    "    #print(TempReindex)\n",
    "    #reindexed = 624#TempReindex.shape[0]\n",
    "    #print(reindexed)\n",
    "    channelOcc = (original/reindexed)*100\n",
    "    #print(channelOcc)\n",
    "    channelO.append(channelOcc)\n",
    "    # Put channel and channelOcc in DF\n",
    "temp_df = pd.DataFrame({'Channel': channel_list, 'perData': channelO})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plt.hist(temp_df['perData'])\n",
    "#plt.xlabel(\"% Data Available\")\n",
    "#plt.ylabel(\"Channel Frequency\")\n",
    "#plt.title(\"Percent data available for the month of October\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataThresh = np.percentile(channelO, 75)\n",
    "\n",
    "temp_df = temp_df[temp_df['perData'] >= dataThresh]\n",
    "channel_list = temp_df['Channel'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged = pd.merge(temp_df,data, on=['Channel','Channel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged = merged.drop('perData', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dropping the data point if null value is encountered in \n",
    "# Occupancy, Power_dbm or SNR. how = 'any' is the default\n",
    "# option which means if any of them is NA, drop the entire\n",
    "# row of the data from the dataset. \n",
    "merged = merged.dropna(subset=['Occupancy', 'Power_dbm', 'SNR'], how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TimeInducedDfOriginal = merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converting the string StartTime to unique float value\n",
    "merged['StartTime'] = pd.to_datetime(merged['StartTime'])\n",
    "merged['StartTime'] = merged['StartTime'].apply(lambda v: v.timestamp())\n",
    "\n",
    "# Converting the string EndTime to unique float value\n",
    "merged['EndTime'] = pd.to_datetime(merged['EndTime']) \n",
    "merged['EndTime'] = merged['EndTime'].apply(lambda v: v.timestamp())\n",
    "\n",
    "# Convert the occupancy into integer value\n",
    "merged['Occupancy'] = merged['Occupancy'].astype(np.int64)\n",
    "\n",
    "#### data['OccupancyClass'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plt.hist(merged['Occupancy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This will label the dataframe on Occupancy and save this new column in the dataframe. \n",
    "# Occupancy percent < 50 will be classified as 0\n",
    "# Occupancy percent > 50 will be classified as 1 \n",
    "merged['OccupancyClass'] = pd.cut(merged['Occupancy'], [-np.inf, 50, np.inf], labels=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer, LabelEncoder, OneHotEncoder, StandardScaler\n",
    "\n",
    "# number of splits in K-fold\n",
    "numOfSplits = 10\n",
    "\n",
    "# iterating for each channel in channel list\n",
    "for ch in channel_list:\n",
    "    print('CHANNEL PICKED FOR PREDICTION:{:.0f}'.format(ch))\n",
    "    print(\"********************************************************\")\n",
    "    \n",
    "    # split the dataframe into two sets: x - attributes, y - response variables\n",
    "    # in our case, y = Occupancy, x would be rest of the columns\n",
    "    layer3dataPerChannelx = merged[(merged.Channel == ch)][['StartTime', 'EndTime', 'Channel', 'Power_dbm','SNR']]\n",
    "    layer3dataPerChannely = merged[(merged.Channel == ch)][['OccupancyClass']]  \n",
    "    \n",
    "        \n",
    "    # initialize variables for each channel\n",
    "    n = len(layer3dataPerChannelx)\n",
    "    kf = KFold(n_splits = numOfSplits)\n",
    "    fold = 0\n",
    "    LRaccuracy = []\n",
    "    NBaccuracy = []\n",
    "    LRFullaccuracy = []\n",
    "    NBFullaccuracy = []\n",
    "    FoldCount = []\n",
    "    \n",
    "    # IF condition is required to make an exception for the channels\n",
    "    # where enough data is not available to create numOfSplits folds\n",
    "    if n > numOfSplits: \n",
    "        for train_idx, test_idx in kf.split(layer3dataPerChannelx):\n",
    "            #print('Train: %s | test: %s' % (train_idx, test_idx))\n",
    "            print('Train length: %s | test length: %s' % (len(train_idx), len(test_idx))) #464, 52\n",
    "            X_train, X_test = layer3dataPerChannelx.iloc[train_idx], layer3dataPerChannelx.iloc[test_idx]\n",
    "            y_train, y_test = layer3dataPerChannely.iloc[train_idx], layer3dataPerChannely.iloc[test_idx]\n",
    "\n",
    "           \n",
    "            X_trainNew, X_testNew = layer3dataPerChannelx.iloc[train_idx, 0:4], layer3dataPerChannelx.iloc[test_idx, 0:4]\n",
    "            y_trainNew, y_testNew = layer3dataPerChannely.iloc[train_idx], layer3dataPerChannely.iloc[test_idx]\n",
    "            \n",
    "            # check if values in y_train are more than 1. \n",
    "            if len(y_train.OccupancyClass.unique()) > 1:\n",
    "                \n",
    "                fold += 1\n",
    "                FoldCount.append(fold)\n",
    "\n",
    "                # Logistic Regression\n",
    "                clf = LogisticRegression().fit(X_train, y_train)\n",
    "                y_pred = clf.predict(X_test)\n",
    "                \n",
    "                scoreLR = clf.score(X_test, y_test)\n",
    "                print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(scoreLR))\n",
    "                LRFullaccuracy.append(scoreLR)\n",
    "                \n",
    "\n",
    "                # Confusion Matrix\n",
    "                cfm_LR = confusion_matrix(y_test, y_pred)\n",
    "                print(\"********************************************************\")\n",
    "                print(\"******* CONFUSION MATRIX FOR LOGISTIC REGRESSION *******\")\n",
    "                print(\"********************************************************\")\n",
    "                print(cfm_LR)\n",
    "\n",
    "                #Create a Naive Bayes Classifier\n",
    "                model = GaussianNB()\n",
    "                # Train the model using the training sets \n",
    "                model.fit(X_train, y_train)\n",
    "                #Predict Output \n",
    "                predicted = model.predict(X_test)\n",
    "                scoreNB = accuracy_score(y_test, predicted)\n",
    "                print('Accuracy of Naive Bayes classifier on test set: {:.2f}'.format(scoreNB))\n",
    "                NBFullaccuracy.append(scoreNB)\n",
    "                \n",
    "                # Confusion Matrix\n",
    "                cfm_NB = confusion_matrix(y_test, y_pred)\n",
    "                print(\"********************************************************\")\n",
    "                print(\"********** CONFUSION MATRIX FOR NAIVE BAYES **********\")\n",
    "                print(\"********************************************************\")\n",
    "                print(cfm_NB)\n",
    "                \n",
    "               \n",
    "                # Logistic Regression for reduced dataset\n",
    "                regr = LogisticRegression()\n",
    "                #clf2 = regr.fit(X_trainNew, np.ravel(y_trainNew,order='C'))\n",
    "                clf2 = regr.fit(X_trainNew, y_trainNew)\n",
    "                y_predNew = clf2.predict(X_testNew)\n",
    "                scoreLRRed = clf2.score(X_testNew, y_testNew)\n",
    "                print('Accuracy of logistic regression classifier on reduced test set: {:.2f}'.format(scoreLRRed))\n",
    "                LRaccuracy.append(scoreLRRed)  \n",
    "\n",
    "                # Naive Bayes for reduced dataset\n",
    "                modelRed = GaussianNB()\n",
    "                # Train the model using the training sets \n",
    "                modelRed.fit(X_trainNew, y_trainNew)\n",
    "                #Predict Output \n",
    "                predictedRed = modelRed.predict(X_testNew)\n",
    "                scoreNBRed = accuracy_score(y_testNew, predictedRed)\n",
    "                print('Accuracy of Naive Bayes classifier on reduced test set: {:.2f}'.format(scoreNBRed))\n",
    "                NBaccuracy.append(scoreNBRed)\n",
    "                \n",
    "                \n",
    "                ###############  TO BE ADDED: TEMPORAL COLUMNS FOR PREDICTION #################\n",
    "                                                       \n",
    "                ###############################################################################\n",
    "                \n",
    "\n",
    "                \n",
    "            else:\n",
    "                print('Training data contains only one class for channel: {:.0f}'.format(ch))\n",
    "        \n",
    "        # Plotting the graph between validation set and \n",
    "        # accuracy for both the algorithms in order to \n",
    "        # compare them. \n",
    "        if len(y_train.OccupancyClass.unique()) > 1:\n",
    "            plt.scatter(FoldCount, LRaccuracy, color = 'blue')\n",
    "            plt.scatter(FoldCount, NBaccuracy, color = 'red')\n",
    "            plt.xlabel('Validation Set')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.title('For Channel:{:.0f}'.format(ch))\n",
    "            plt.show()\n",
    "            \n",
    "            plt.plot(FoldCount, LRaccuracy, color = 'purple')\n",
    "            plt.plot(FoldCount, LRFullaccuracy, color = 'pink')\n",
    "            plt.xlabel('Validation Set')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.title('Logistic Regression: Full v/s Reduced dataset For Channel:{:.0f}'.format(ch))\n",
    "            plt.show()\n",
    "\n",
    "            plt.plot(FoldCount, NBaccuracy, color = 'yellow')\n",
    "            plt.plot(FoldCount, NBFullaccuracy, color = 'green')\n",
    "            plt.xlabel('Validation Set')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.title('Naive Bayes: Full v/s Reduced dataset For Channel:{:.0f}'.format(ch))\n",
    "            plt.show()\n",
    "\n",
    "    else:\n",
    "        print('Not enough data is available to run the model for channel: {:.0f}'.format(ch))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some inferences from the results:\n",
    "\n",
    "#### 1. Reducing the columns (by removing SNR and Power_dbm) had no role in prediction. The predicted outcome was same whether or not columns were present when ran with either Logistic Regression or Naive Bayes. This implies for all our predictions we can safely ignore SNR and Power_dbm attributes. <br>\n",
    "\n",
    "#### 2. For channels where the predicion rate was low, logistic regression performed better as compared to Naive Bayes. However, for channels where high prediction rate was achieved, both algorithm predicted similar results. This implies overall Logistic Regression performed better than Naive Bayes. </br>\n",
    "\n",
    "#### 3. The code is ran on 7 channels- 56269, 9000, 50959, 55227, 54454, 51181, 103222. This include mix of channels from the following areas:\n",
    "##### 3.a. 56269, 50959: there is only one class in training response variable (since occupancy for all occurences is 0). Such channels are skipped over in prediction.\n",
    "##### 3.b. 103222: number of splits in k-fold is greater than number of samples. Such channels are also handled by skipping over them. \n",
    "##### *3.c.* 9000, 55227, 54454, 51181: these channels have enough data as well as prediction values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
